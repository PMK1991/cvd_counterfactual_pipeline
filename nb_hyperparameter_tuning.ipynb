{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers # helper functions\n",
    "from dice_ml import Data,Model,Dice\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "import threading\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import numpy as np\n",
    "from dataLoader import DataLoader\n",
    "from plotter import Plotter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters: {'classifier__colsample_bytree': 1.0, 'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 300, 'classifier__subsample': 0.8}\n",
      "Best Score: 0.8564724269401216\n",
      "Best Estimator: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['age', 'trestbps', 'chol',\n",
      "                                                   'thalach', 'oldpeak']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('onehot',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  Index(['cp', 'exang', 'fbs', 'restecg', 'sex', 'slope'], dtype='object'))])),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=N...\n",
      "                               feature_types=None, gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=0.01,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=3, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=300, n_jobs=None,\n",
      "                               num_parallel_tree=None, random_state=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, model, param_grid, scoring='precision', cv=5, verbose=1, n_jobs=-1):\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "        self.grid_search = None\n",
    "\n",
    "    def tune(self, X, y):\n",
    "        self.grid_search = GridSearchCV(estimator=self.model, param_grid=self.param_grid, scoring=self.scoring, cv=self.cv, verbose=self.verbose, n_jobs=self.n_jobs)\n",
    "        self.grid_search.fit(X, y)\n",
    "        return self.grid_search.best_params_, self.grid_search.best_score_\n",
    "\n",
    "    def get_best_estimator(self):\n",
    "        if self.grid_search:\n",
    "            return self.grid_search.best_estimator_\n",
    "        else:\n",
    "            raise ValueError(\"You need to run the tune method first.\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset using DataLoader\n",
    "    dataLoader = DataLoader(\"heart_statlog_cleveland_hungary_final.csv\")\n",
    "    df_cvd = dataLoader.load_data()\n",
    "\n",
    "    # Assuming the target column is named 'target'\n",
    "    X = df_cvd.drop(columns='target')\n",
    "    y = df_cvd['target']\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    numerical = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "    categorical = X_train.columns.difference(numerical)\n",
    "\n",
    "\n",
    "    # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical),\n",
    "            ('cat', categorical_transformer, categorical)])\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', transformations),\n",
    "    ('classifier', XGBClassifier())\n",
    "   ])\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [3, 6, 9],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__subsample': [0.8, 1.0],\n",
    "        'classifier__colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    # Create the HyperparameterTuner instance\n",
    "    tuner = HyperparameterTuner(pipeline, param_grid)\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    best_params, best_score = tuner.tune(X_train, y_train)\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Score:\", best_score)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_model = tuner.get_best_estimator()\n",
    "    print(\"Best Estimator:\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9066666666666666\n",
      "Test F1 Score: 0.9067330842673308\n",
      "Test Precision: 0.908088888888889\n",
      "Test Recall: 0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the best hyperparameters on the full training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test F1 Score: {f1}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numerical = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "categorical = X_train.columns.difference(numerical)\n",
    "\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        ('cat', categorical_transformer, categorical)])\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', transformations),('classifier', XGBClassifier(max_depth=3, learning_rate=0.01, n_estimators=300, subsample=0.8,colsample_bytree=1.0))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_risk_tp = X_test[(y_pred == 1) & (y_test == 1)].reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_high_risk_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
