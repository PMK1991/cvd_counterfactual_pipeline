{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.11.9' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers # helper functions\n",
    "from dice_ml import Data,Model,Dice\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "import threading\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "#from langchain.llms import AzureOpenAI\n",
    "#from langchain_openai import AzureChatOpenAI\n",
    "import pandas as pd \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_heart_disease = pd.read_csv(\"heart_statlog_cleveland_hungary_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataframe_heart_disease =  dataframe_heart_disease.dropna()\n",
    "dataframe_heart_disease =  dataframe_heart_disease.drop_duplicates()\n",
    "dataframe_heart_disease = dataframe_heart_disease[dataframe_heart_disease['chol'] !=0]\n",
    "dataframe_heart_disease = dataframe_heart_disease[dataframe_heart_disease['trestbps'] !=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target label\n",
    "y = dataframe_heart_disease.target\n",
    "X = dataframe_heart_disease.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "categorical = X_train.columns.difference(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        ('cat', categorical_transformer, categorical)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', transformations),\n",
    "                      ('classifier', XGBClassifier())])\n",
    "xgb_model = clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'classifier__max_depth': [3, 4, 5],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.5],\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__gamma': [0, 0.1, 0.5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(xgb_model, param_grid=param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters and the score on the test set\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Test set score: \", grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'classifier__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "    'classifier__n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'classifier__gamma': [0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__min_child_weight': [1, 2, 3, 4, 5],\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1, 1, 10],\n",
    "    'classifier__reg_lambda': [0, 0.01, 0.1, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(xgb_model, param_grid=param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters and the score on the test set\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Test set score: \", grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the parameter space for Bayesian search\n",
    "param_space = {\n",
    "    'classifier__max_depth': (3, 10),\n",
    "    'classifier__learning_rate': (0.001, 0.5, 'log-uniform'),\n",
    "    'classifier__n_estimators': (50, 500),\n",
    "    'classifier__gamma': (0, 0.5, 'uniform'),\n",
    "    'classifier__subsample': (0.6, 1.0, 'uniform'),\n",
    "    'classifier__colsample_bytree': (0.6, 1.0, 'uniform'),\n",
    "    'classifier__min_child_weight': (1, 5),\n",
    "    'classifier__reg_alpha': (0, 10, 'log-uniform'),\n",
    "    'classifier__reg_lambda': (0, 10, 'log-uniform')\n",
    "}\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', transformations),\n",
    "                      ('classifier', XGBClassifier())])\n",
    "\n",
    "# Perform Bayesian search\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=clf,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=50,  # Number of iterations\n",
    "    cv=5,  # Cross-validation splitting strategy\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model using Bayesian search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", bayes_search.best_params_)\n",
    "\n",
    "# Use the best estimator for predictions\n",
    "best_model = bayes_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline with the XGBClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', transformations),\n",
    "    ('classifier', XGBClassifier(max_depth=5, learning_rate=0.5, n_estimators=200, gamma=0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the entire training set\n",
    "xgb_pipeline = pipeline.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = xgb_pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_risk_tp = X_test[(y_pred == 1) & (y_test == 1)].reset_index().drop(['index'], axis=1)\n",
    "#X_high_risk_tp = X_test[(y_pred == 1) & (y_test == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "\n",
    "# Create a DICE data object\n",
    "d = Data(dataframe=pd.DataFrame(train_data, columns=dataframe_heart_disease.columns), continuous_features=['age', 'trestbps', 'chol', 'thalach', 'oldpeak'],outcome_name='target')\n",
    "\n",
    "# Create a DICE model object\n",
    "m = Model(model=xgb_pipeline, backend=\"sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal Countefactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "\n",
    "df_no_counterfactuals = pd.DataFrame(columns=X_high_risk_tp.columns)\n",
    "def generate_cf(test_instance, timeout=10):\n",
    "    q = queue.Queue()\n",
    "\n",
    "    def target():\n",
    "        try:\n",
    "            result = Dice(d, m, method='genetic').generate_counterfactuals(test_instance, total_CFs=3, desired_class=\"opposite\",\n",
    "                                                                           features_to_vary=[\"trestbps\", \"chol\", \"thalach\"],\n",
    "                                                                           #diversity_weight=5, proximity_weight=2, sparsity_weight=5,\n",
    "                                                                           permitted_range={\"trestbps\": [80, 120], \"chol\": [150, 200], \"thalach\": [120, 220 - test_instance['age'].values[0]]}\n",
    "                                                                           )\n",
    "            q.put(result)\n",
    "        except Exception as e:\n",
    "            print(\"No counterfactuals found for test instance:\", test_instance)\n",
    "            df_no_counterfactuals.append(test_instance)\n",
    "            q.put(None)\n",
    "\n",
    "    # Start a new thread to run the target function\n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.start()\n",
    "\n",
    "    # Wait for the thread to finish or raise a timeout exception\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        # The thread is still running, so raise a timeout exception\n",
    "        print(\"No counterfactuals found for test instance-timed out:\", test_instance)\n",
    "        df_no_counterfactuals.append(test_instance)\n",
    "        q.put(None)\n",
    "    else:\n",
    "        # The thread has finished, so return the result\n",
    "        return q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exps_ideal = []\n",
    "num_cores = -1\n",
    "\n",
    "# Iterate over each instance of X_high_risk and generate counterfactuals\n",
    "for i in range(len(X_high_risk_tp)):\n",
    "    test_instance = X_high_risk_tp.iloc[[i]]\n",
    "    print(i)\n",
    "    exp = Parallel(n_jobs=num_cores)(delayed(generate_cf)(test_instance) for i in range(1))\n",
    "    exps_ideal.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_none = sum(exp[0] is None for exp in exps_ideal)\n",
    "print(\"Number of None values in exps_ideal:\", num_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((96-18)/96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validity_list = []\n",
    "for exps in exps_ideal:\n",
    "    \n",
    "    if exps[0] is not None:\n",
    "       exp_df = exps[0].cf_examples_list[0].final_cfs_df\n",
    "       validity_list.append(xgb_pipeline.predict(exp_df)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values_list = []\n",
    "median_values_list = []\n",
    "std_values_list = []\n",
    "\n",
    "for exps in exps_ideal:\n",
    "    if exps[0] is not None:\n",
    "        exp_df = exps[0].cf_examples_list[0].final_cfs_df\n",
    "\n",
    "        # Calculate the mean\n",
    "        mean_values = exp_df[['chol', 'trestbps', 'thalach']].mean()\n",
    "        mean_values_list.append(mean_values)\n",
    "\n",
    "        # Calculate the median\n",
    "        median_values = exp_df[['chol', 'trestbps', 'thalach']].median()\n",
    "        median_values_list.append(median_values)\n",
    "\n",
    "        # Calculate the standard deviation\n",
    "        std_values = exp_df[['chol', 'trestbps', 'thalach']].std()\n",
    "        std_values_list.append(std_values)\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "mean_values_df = pd.DataFrame(mean_values_list)\n",
    "median_values_df = pd.DataFrame(median_values_list)\n",
    "std_values_df = pd.DataFrame(std_values_list)\n",
    "\n",
    "# Calculate and print the mean of means, median of medians, and std dev of std devs\n",
    "print('Mean of means:\\n', mean_values_df.mean())\n",
    "print('\\nMedian of medians:\\n', median_values_df.median())\n",
    "print('\\nStandard deviation of standard deviations:\\n', std_values_df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "all_data_df = pd.DataFrame()\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "for exps in exps_ideal:\n",
    "    if exps[0] is not None:\n",
    "        exp_df = exps[0].cf_examples_list[0].final_cfs_df\n",
    "        all_data_df = pd.concat([all_data_df, exp_df[['chol', 'trestbps', 'thalach']]])\n",
    "\n",
    "\n",
    "# Calculate and store the mean, median, and std dev\n",
    "for column in ['chol', 'trestbps', 'thalach']:\n",
    "    statistics[column] = {\n",
    "        'mean': np.mean(all_data_df[column]),\n",
    "        'median': np.median(all_data_df[column]),\n",
    "        'std_dev': np.std(all_data_df[column])\n",
    "    }\n",
    "\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "statistics_df = pd.DataFrame(statistics)\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot the mean values\n",
    "plt.subplot(1, 3, 1)\n",
    "statistics_df.loc['mean'].plot(kind='bar')\n",
    "plt.title('Mean')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Plot the median values\n",
    "plt.subplot(1, 3, 2)\n",
    "statistics_df.loc['median'].plot(kind='bar')\n",
    "plt.title('Median')\n",
    "\n",
    "# Plot the standard deviation values\n",
    "plt.subplot(1, 3, 3)\n",
    "statistics_df.loc['std_dev'].plot(kind='bar')\n",
    "plt.title('Standard Deviation')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_ideal[0][0].visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = sum(validity_list) / len(validity_list) * 100\n",
    "print(\"Number of Valid Ideal Counterfactuals: {:.2f}%\".format(100-percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tp = X_high_risk_tp.copy()  # Create a copy of the DataFrame to avoid modifying the original one\n",
    "df_tp['target'] = 1  # Append a new column 'target' with all values as 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_data():\n",
    "    dict_strings = []\n",
    "    #df_tp = pd.read_csv(\"heart_statlog_cleveland_hungary_final.csv\")\n",
    "    # Update 'sex' column\n",
    "    df_tp['sex'] = df_tp['sex'].replace({1: 'Male', 0: 'Female'})\n",
    "    # Update 'cp' column\n",
    "    df_tp['cp'] = df_tp['cp'].replace({1: 'typical angina', 2: 'atypical angina', 3: 'non-anginal pain', 4: 'asymptomatic'})\n",
    "    # Update 'fbs' column\n",
    "    df_tp['fbs'] = df_tp['fbs'].replace({1: 'True', 0: 'False'})\n",
    "    # Update 'restecg' column\n",
    "    df_tp['restecg'] = df_tp['restecg'].replace({0: 'normal', 1: 'ST-T wave abnormality', 2: 'probable or definite left ventricular hypertrophy'})\n",
    "    # Update 'exang' column\n",
    "    df_tp['exang'] = df_tp['exang'].replace({1: 'yes', 0: 'no'})\n",
    "    # Update 'slope' column\n",
    "    df_tp['slope'] = df_tp['slope'].replace({1: 'upsloping', 2: 'flat', 3: 'downsloping'})\n",
    "    # Update 'target' column\n",
    "    df_tp['target'] = df_tp['target'].replace({1: 'yes', 0: 'no'})\n",
    "    df_tp_risk = df_tp[df_tp['target'] == 'yes'].reset_index(drop=True)\n",
    "    for i in range(len(df_tp_risk)):\n",
    "        patient_records = df_tp_risk.iloc[i]\n",
    "        dict_string = json.dumps(patient_records.to_dict())\n",
    "        dict_strings.append(dict_string)\n",
    "    return dict_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "\n",
    "    \n",
    "def create_patient_description (patient_record):\n",
    "    # Convert the patient record to a dictionary string\n",
    "    \n",
    "    #print(i)\n",
    "    dict_string = json.dumps(patient_record)\n",
    "\n",
    "    # Create the message text\n",
    "    message_text = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"You are a helpful assistant. \n",
    "        Describe this patient info\n",
    "        ---\n",
    "        {dict_string} \n",
    "        ---\n",
    "        in plain english for me. Be precise and concise.\n",
    "        \"\"\"}]\n",
    "\n",
    "    # Create the chat completion\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-32k-deployment\",\n",
    "        messages=message_text,\n",
    "        temperature=0.0,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    # Return the generated description\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_patient_description_with_retry(patient_record):\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return create_patient_description(patient_record)\n",
    "        except Exception as e:\n",
    "            if '429' in str(e):\n",
    "                print(f\"Rate limit exceeded. Retrying after {2**retries} seconds.\")\n",
    "                time.sleep(2**retries)\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                return None\n",
    "\n",
    "def create_patient_descriptions(patient_records):\n",
    "    # Create a thread pool executor\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        patient_descriptions = list(executor.map(create_patient_description_with_retry, patient_records))\n",
    "\n",
    "    return patient_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_descriptions = []\n",
    "for i in range(len(patient_data)):\n",
    "    patient_record = json.loads(patient_data[i])\n",
    "    patient_description = create_patient_description(patient_record)\n",
    "    patient_descriptions.append(patient_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_cf_ideal (patient_record):\n",
    "    # Convert the patient record to a dictionary string \n",
    "\n",
    "    message_text = [{\"role\":\"system\",\n",
    "        \"content\":f\"\"\"You are a helpful assistant in Counterfactual Reasoning. \n",
    "        Here are the patient details:\n",
    "        --- \n",
    "        {patient_record}\n",
    "        ---\n",
    "        What should the patient do specifically to prevent heart disease?\n",
    "        Provide only three counterfactuals in precise comma separated manner in the following json format:\n",
    "        ---\n",
    "        {{\"Preventive Measures\": [[{{\"Cholestrol\":\"chol1\"}},{{\"Resting Blood Pressure\":\"trestbps1\"}},{{\"Max Heart Rate\":\"thalach1\"}}],\n",
    "        [{{\"Cholestrol\":\"chol2\"}},{{\"Resting Blood Pressure\":\"trestbps2\"}},{{\"Max Heart Rate\":\"thalach2\"}}],\n",
    "        [{{\"Cholestrol\":\"chol3\"}},{{\"Resting Blood Pressure\":\"trestbps3\"}},{{\"Max Heart Rate\":\"thalach3\"}}]]}}\n",
    "        ---\n",
    "        \n",
    "        Generate three counterfactuals where the value for 'Cholestrol' is less than the value for 'chol' and also less than 200, the value for 'Resting Blood Pressure' is less than the value for 'trestbps' and also less than 120, and the value for 'Max Heart Rate' is greater than the value for 'thalach' and less than the difference between 220 and the patient's age.For example, if a patient's 'thalach' is 150 and their age is 50, the 'Max Heart Rate' in the counterfactual should be a number greater than 150 but less than 170 (which is 220 minus 50).\n",
    "        Satisfy all the constraints and provide the counterfactuals.\n",
    "        Be consitent in formatting. Don't provide general advice and say don't know if you can't provide a specific number.\n",
    "        \"\"\"}]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "            model=\"gpt-4-32k-deployment\",\n",
    "            messages=message_text,\n",
    "            temperature=0.0,\n",
    "            max_tokens=800,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None\n",
    "        )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_cvd_counterfactual_ideal = []\n",
    "for idx,patient_record in enumerate(patient_descriptions):\n",
    "    print(idx)\n",
    "    patient_cvd_counterfactual_ideal.append(create_patient_cf_ideal(patient_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_validity(patient_cvd_counterfactual_ideal,xgb_pipeline):\n",
    "    \n",
    "    validity_list = []\n",
    "\n",
    "    for i in range(len(patient_cvd_counterfactual_ideal)):\n",
    "\n",
    "        data = json.loads(patient_cvd_counterfactual_ideal[i])['Preventive Measures']\n",
    "        # Flatten each sub-list into a single dictionary\n",
    "        flattened_data = [dict(item for d in sublist for item in d.items()) for sublist in data]\n",
    "\n",
    "        # Convert the list of dictionaries into a DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "\n",
    "        # Get the first instance of X_high_risk_tp\n",
    "        first_instance = X_high_risk_tp.iloc[i]\n",
    "        counterfactuals_df = pd.DataFrame()\n",
    "        \n",
    "        \n",
    "\n",
    "        # Create three counterfactuals\n",
    "        for i in range(len(df)):\n",
    "            # Copy the first instance\n",
    "            counterfactual = first_instance.copy()\n",
    "            \n",
    "            # Update the values\n",
    "            counterfactual['trestbps'] = df.iloc[i]['Resting Blood Pressure']\n",
    "            counterfactual['chol'] = df.iloc[i]['Cholestrol']\n",
    "            counterfactual['thalach'] = df.iloc[i]['Max Heart Rate']\n",
    "            \n",
    "            # Append the counterfactual to X_high_risk_tp\n",
    "            #X_high_risk_tp = X_high_risk_tp.append(counterfactual, ignore_index=True)\n",
    "            counterfactuals_df = counterfactuals_df.append(counterfactual, ignore_index=True)\n",
    "\n",
    "        validity_list.append(xgb_pipeline.predict(counterfactuals_df)[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(len(validity_list))\n",
    "    percentage = sum(validity_list) / len(validity_list) * 100\n",
    "\n",
    "    return percentage,validity_list\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(patient_cvd_counterfactual_ideal[3])['Preventive Measures']\n",
    "# Flatten each sub-list into a single dictionary\n",
    "flattened_data = [dict(item for d in sublist for item in d.items()) for sublist in data]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(flattened_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals_df = pd.DataFrame()\n",
    "        \n",
    "        \n",
    "first_instance = X_high_risk_tp.iloc[3]\n",
    "# Create three counterfactuals\n",
    "for i in range(len(df)):\n",
    "    # Copy the first instance\n",
    "    counterfactual = first_instance.copy()\n",
    "    \n",
    "    # Update the values\n",
    "    counterfactual['trestbps'] = df.iloc[i]['Resting Blood Pressure']\n",
    "    counterfactual['chol'] = df.iloc[i]['Cholestrol']\n",
    "    counterfactual['thalach'] = df.iloc[i]['Max Heart Rate']\n",
    "    \n",
    "    # Append the counterfactual to X_high_risk_tp\n",
    "    #X_high_risk_tp = X_high_risk_tp.append(counterfactual, ignore_index=True)\n",
    "    counterfactuals_df = counterfactuals_df.append(counterfactual, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.predict(counterfactuals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_cvd_counterfactual_ideal[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validity_cfs,validity_list = get_validity(patient_cvd_counterfactual_ideal,xgb_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_risk_tp[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_risk_tp[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "selected_data = []\n",
    "\n",
    "for patient in patient_cvd_counterfactual_ideal:\n",
    "    patient_dict = json.loads(patient)\n",
    "    measures = [item for sublist in patient_dict['Preventive Measures'] for item in sublist]\n",
    "    cholestrol_values = [measure['Cholestrol'] for measure in measures if 'Cholestrol' in measure]\n",
    "    blood_pressure_values = [measure['Resting Blood Pressure'] for measure in measures if 'Resting Blood Pressure' in measure]\n",
    "    heart_rate_values = [measure['Max Heart Rate'] for measure in measures if 'Max Heart Rate' in measure]\n",
    "    selected_data.append({\n",
    "        'Cholestrol': cholestrol_values,\n",
    "        'Resting Blood Pressure': blood_pressure_values,\n",
    "        'Max Heart Rate': heart_rate_values\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "statistics_llm = {}\n",
    "\n",
    "for key in ['Cholestrol', 'Resting Blood Pressure', 'Max Heart Rate']:\n",
    "    values = [int(value) for sublist in [patient[key] for patient in selected_data] for value in sublist]\n",
    "    statistics_llm[key] = {\n",
    "        'mean': np.mean(values),\n",
    "        'median': np.median(values),\n",
    "        'std_dev': np.std(values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "statistics_df_llm = pd.DataFrame(statistics_llm)\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot the mean values\n",
    "plt.subplot(1, 3, 1)\n",
    "statistics_df_llm.loc['mean'].plot(kind='bar')\n",
    "plt.title('Mean')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Plot the median values\n",
    "plt.subplot(1, 3, 2)\n",
    "statistics_df_llm.loc['median'].plot(kind='bar')\n",
    "plt.title('Median')\n",
    "\n",
    "# Plot the standard deviation values\n",
    "plt.subplot(1, 3, 3)\n",
    "statistics_df_llm.loc['std_dev'].plot(kind='bar')\n",
    "plt.title('Standard Deviation')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the lists to pandas Series\n",
    "cholesterol_series = pd.Series(cholesterol)\n",
    "resting_bp_series = pd.Series(resting_bp)\n",
    "max_heart_rate_series = pd.Series(max_heart_rate)\n",
    "\n",
    "# Calculate the mean\n",
    "cholesterol_mean = cholesterol_series.mean()\n",
    "resting_bp_mean = resting_bp_series.mean()\n",
    "max_heart_rate_mean = max_heart_rate_series.mean()\n",
    "\n",
    "# Calculate the median\n",
    "cholesterol_median = cholesterol_series.median()\n",
    "resting_bp_median = resting_bp_series.median()\n",
    "max_heart_rate_median = max_heart_rate_series.median()\n",
    "\n",
    "# Calculate the standard deviation\n",
    "cholesterol_std = cholesterol_series.std()\n",
    "resting_bp_std = resting_bp_series.std()\n",
    "max_heart_rate_std = max_heart_rate_series.std()\n",
    "\n",
    "# Now the variables cholesterol_mean, resting_bp_mean, max_heart_rate_mean, cholesterol_median, resting_bp_median, max_heart_rate_median, cholesterol_std, resting_bp_std, and max_heart_rate_std contain the calculated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Single Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean values\n",
    "print(\"Mean values:\")\n",
    "print(\"Cholesterol:\", cholesterol_mean)\n",
    "print(\"Resting BP:\", resting_bp_mean)\n",
    "print(\"Max Heart Rate:\", max_heart_rate_mean)\n",
    "\n",
    "# Print the median values\n",
    "print(\"\\nMedian values:\")\n",
    "print(\"Cholesterol:\", cholesterol_median)\n",
    "print(\"Resting BP:\", resting_bp_median)\n",
    "print(\"Max Heart Rate:\", max_heart_rate_median)\n",
    "\n",
    "# Print the standard deviation values\n",
    "print(\"\\nStandard deviation values:\")\n",
    "print(\"Cholesterol:\", cholesterol_std)\n",
    "print(\"Resting BP:\", resting_bp_std)\n",
    "print(\"Max Heart Rate:\", max_heart_rate_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feasible Counterfactuals -  For the instances that did not have Ideal CFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_risk_tp = X_test[(y_pred == 1) & (y_test == 1)].reset_index().drop(['index'], axis=1)\n",
    "#df_no_counterfactuals = X_high_risk_tp.loc[[exp[0] is None for exp in exps_ideal]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "\n",
    "df_no_feasible_counterfactuals = pd.DataFrame(columns=df_no_counterfactuals.columns)\n",
    "\n",
    "def generate_cf_feasible(test_instance, timeout=30):\n",
    "    q = queue.Queue()\n",
    "\n",
    "    def target():\n",
    "        try:\n",
    "            result = Dice(d, m, method='genetic').generate_counterfactuals(test_instance, total_CFs=5, desired_class=\"opposite\",\n",
    "                                                                           features_to_vary=[\"trestbps\", \"chol\", \"thalach\"],\n",
    "                                                                           #diversity_weight=5, proximity_weight=2, sparsity_weight=5,\n",
    "                                                                           permitted_range={\"trestbps\": [80, 149],\n",
    "                                                                                            \"chol\": [150, test_instance['chol'].values[0]-0.1*test_instance['chol'].values[0]],\n",
    "                                                                                            \"thalach\": [120, 220 - test_instance['age'].values[0]]}                                                                           )\n",
    "            q.put(result)\n",
    "        except Exception as e:\n",
    "            print(\"No counterfactuals found for test instance:\", test_instance)\n",
    "            df_no_feasible_counterfactuals.append(test_instance)\n",
    "            q.put(None)\n",
    "\n",
    "    # Start a new thread to run the target function\n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.start()\n",
    "\n",
    "    # Wait for the thread to finish or raise a timeout exception\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        # The thread is still running, so raise a timeout exception\n",
    "        print(\"No counterfactuals found for test instance-timed out:\", test_instance)\n",
    "        df_no_feasible_counterfactuals.append(test_instance)\n",
    "        q.put(None)\n",
    "    else:\n",
    "        # The thread has finished, so return the result\n",
    "        return q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_feasible = []\n",
    "num_cores = -1\n",
    "df_no_counterfactuals_feasible = pd.DataFrame(columns=X_high_risk_tp.columns)\n",
    "# Iterate over each instance of X_high_risk and generate counterfactuals\n",
    "for i in range(len(X_high_risk_tp)):\n",
    "    test_instance = X_high_risk_tp.iloc[[i]]\n",
    "    print(i)   \n",
    "    exp = Parallel(n_jobs=num_cores)(delayed(generate_cf_feasible)(test_instance) for i in range(1))\n",
    "    exps_feasible.append(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_none = sum(exp[0] is None for exp in exps_feasible)\n",
    "print(\"Number of None values in exps_feasible:\", num_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feasible_validity_list = []\n",
    "for exps in exps_feasible:\n",
    "    \n",
    "    if exps[0] is not None:\n",
    "       exp_df = exps[0].cf_examples_list[0].final_cfs_df\n",
    "       feasible_validity_list.append(xgb_pipeline.predict(exp_df)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_feasible[0][0].visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = sum(feasible_validity_list) / len(feasible_validity_list) * 100\n",
    "print(\"Number of Valid Feasible Counterfactuals: {:.2f}%\".format(100-percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_no_counterfactuals_feasible = X_test_no_ideal_cf.loc[[exp[0] is None for exp in exps_feasible]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_no_counterfactuals_feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General CFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_risk_tp = X_test[(y_pred == 1) & (y_test == 1)].reset_index().drop(['index'], axis=1)\n",
    "#df_no_counterfactuals = X_high_risk_tp.loc[[exp[0] is None for exp in exps_ideal]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "\n",
    "\n",
    "\n",
    "def generate_cf_general(test_instance, timeout=300):\n",
    "    q = queue.Queue()\n",
    "\n",
    "    def target():\n",
    "        try:\n",
    "            result = Dice(d, m, method='genetic').generate_counterfactuals(test_instance, total_CFs=20, desired_class=\"opposite\",\n",
    "                                                                           features_to_vary=[\"trestbps\", \"chol\", \"thalach\"],\n",
    "                                                                           diversity_weight=5, proximity_weight=2, sparsity_weight=5\n",
    "                                                                          )\n",
    "            q.put(result)\n",
    "        except Exception as e:\n",
    "            print(\"No counterfactuals found for test instance:\", test_instance)\n",
    "            #df_no_counterfactuals.append(test_instance)\n",
    "            q.put(None)\n",
    "\n",
    "    # Start a new thread to run the target function\n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.start()\n",
    "\n",
    "    # Wait for the thread to finish or raise a timeout exception\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        # The thread is still running, so raise a timeout exception\n",
    "        print(\"No counterfactuals found for test instance-timed out:\", test_instance)\n",
    "        #df_no_counterfactuals.append(test_instance)\n",
    "        q.put(None)\n",
    "    else:\n",
    "        # The thread has finished, so return the result\n",
    "        return q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_general= []\n",
    "num_cores = -1\n",
    "df_no_counterfactuals_general= pd.DataFrame(columns=X_high_risk_tp.columns)\n",
    "# Iterate over each instance of X_high_risk and generate counterfactuals\n",
    "for i in range(len(X_high_risk_tp)):\n",
    "    test_instance = X_high_risk_tp.iloc[[i]]\n",
    "    print(i)   \n",
    "    exp = Parallel(n_jobs=num_cores)(delayed(generate_cf_general)(test_instance) for i in range(1))\n",
    "    exps_general.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_none = sum(exp[0] is None for exp in exps_general)\n",
    "print(\"Number of None values in exps_general:\", num_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_general[0][0].visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_validity_list = []\n",
    "for exps in exps_general:\n",
    "    \n",
    "    if exps[0] is not None:\n",
    "       exp_df = exps[0].cf_examples_list[0].final_cfs_df\n",
    "       general_validity_list.append(xgb_pipeline.predict(exp_df)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = sum(general_validity_list) / len(general_validity_list) * 100\n",
    "print(\"Number of Valid General Counterfactuals: {:.2f}%\".format(100-percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_high_risk_tp)):\n",
    "     \n",
    "     test_instance = X_high_risk_tp.iloc[[i]]\n",
    "     exp = exps_general[i] \n",
    "\n",
    "     if exp[0] is not None:\n",
    "          exp_dfs = exp[0].cf_examples_list[0].final_cfs_df\n",
    "          thalach_test = test_instance['thalach'].values[0] \n",
    "          chol_test = test_instance['chol'].values[0]\n",
    "\n",
    "\n",
    "          feasible_counterfactuals = exp_dfs[((exp_dfs['thalach'] > thalach_test) & (exp_dfs['thalach']< 220 - exp_dfs['age'])) & (exp_dfs['chol'] <= chol_test-0.1*chol_test) & (exp_dfs['trestbps'] <= 149)]\n",
    "\n",
    "          if len(feasible_counterfactuals) > 0:\n",
    "               print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feasible_counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dfs = exps_general[1][0].cf_examples_list[0].final_cfs_df\n",
    "\n",
    "\n",
    "feasible_counterfactuals = exp_dfs[((exp_dfs['thalach'] > thalach_test) & (exp_dfs['thalach']< 220 - exp_dfs['age'])) & (exp_dfs['chol'] <= 200) & (exp_dfs['trestbps'] <= 120)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feasible_counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_general[1][0].visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Less Feasible Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_high_risk_tp = X_test[(y_pred == 1) & (y_test == 1)].reset_index().drop(['index'], axis=1)\n",
    "#df_no_counterfactuals = X_high_risk_tp.loc[[exp[0] is None for exp in exps_ideal]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "\n",
    "\n",
    "\n",
    "def generate_cf_general_feasible(test_instance, timeout=300):\n",
    "    q = queue.Queue()\n",
    "\n",
    "    def target():\n",
    "        try:\n",
    "            result = Dice(d, m, method='genetic').generate_counterfactuals(test_instance, total_CFs=20, desired_class=\"opposite\",\n",
    "                                                                           features_to_vary=[\"trestbps\", \"chol\", \"thalach\"],\n",
    "                                                                           diversity_weight=5, proximity_weight=2, sparsity_weight=5,\n",
    "                                                                           permitted_range={\"trestbps\": [80, test_instance['age'].values[0]-1],\n",
    "                                                                                            \"chol\": [150, test_instance['chol'].values[0]-1],\n",
    "                                                                                            \"thalach\": [test_instance['thalach'].values[0]+1, 220 - test_instance['age'].values[0]]}\n",
    "                                                                          )\n",
    "            q.put(result)\n",
    "        except Exception as e:\n",
    "            print(\"No counterfactuals found for test instance:\", test_instance)\n",
    "            #df_no_counterfactuals.append(test_instance)\n",
    "            q.put(None)\n",
    "\n",
    "    # Start a new thread to run the target function\n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.start()\n",
    "\n",
    "    # Wait for the thread to finish or raise a timeout exception\n",
    "    thread.join(timeout)\n",
    "\n",
    "    if thread.is_alive():\n",
    "        # The thread is still running, so raise a timeout exception\n",
    "        print(\"No counterfactuals found for test instance-timed out:\", test_instance)\n",
    "        #df_no_counterfactuals.append(test_instance)\n",
    "        q.put(None)\n",
    "    else:\n",
    "        # The thread has finished, so return the result\n",
    "        return q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_general_feasible= []\n",
    "num_cores = -1\n",
    "df_no_counterfactuals_general_feasible= pd.DataFrame(columns=X_high_risk_tp.columns)\n",
    "# Iterate over each instance of X_high_risk and generate counterfactuals\n",
    "for i in range(len(X_high_risk_tp)):\n",
    "    test_instance = X_high_risk_tp.iloc[[i]]\n",
    "    print(i)   \n",
    "    exp = Parallel(n_jobs=num_cores)(delayed(generate_cf_general_feasible)(test_instance) for i in range(1))\n",
    "    exps_general_feasible.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_none = sum(exp[0] is None for exp in exps_general_feasible)\n",
    "print(\"Number of None values in exps_general_feasible:\", num_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_feasible_validity_list = []\n",
    "for exps in exps_general_feasible:\n",
    "    \n",
    "    if exps[0] is not None:\n",
    "       exp_df = exps[0].cf_examples_list[0].final_cfs_df\n",
    "       general_feasible_validity_list.append(xgb_pipeline.predict(exp_df)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = sum(general_feasible_validity_list) / len(general_feasible_validity_list) * 100\n",
    "print(\"Number of Valid General Counterfactuals: {:.2f}%\".format(100-percentage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
